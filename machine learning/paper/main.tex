\documentclass[a4paper, 10pt]{article}
\usepackage{Iv_xstyles}
\tracingmacros = 0
\addbibresource[datatype=bibtex]{papers.bib}
\usepackage{multicol}
\setlength{\columnsep}{0.5cm}

% Version 0.2

\begin{document}
\begin{center}
    \vspace{2.6cm}
    \textsc{\LARGE Синергия алгоритмов классификации (SVM Multimodelling)}
    \vspace{0.6cm}

    { Сергей Иванычев, Александр Адуенко}

\texttt{\small
    \href{mailto:sergeyivanychev@gmail.com}{sergeyivanychev@gmail.com},  \href{mailto:aduenko1@gmail.com}{aduenko1@gmail.com}
}
    \vspace{0.6cm}
    \end{center}
    \noindent\makebox[\linewidth]{\rule{\textwidth}{0.4pt}}

    {\textbf{}\small В данной статье рассматривается проблема агрегирования небольшого количества сильных классификаторов с целью улучшения решений задач классификации и регрессии.  В качестве примера подобной системы рассматривается система SVM  алгоритмов использующая kernel-trick с различными ядрами. Для комбинации решений и улучшения качества прогнозирования в задачах классификации и регрессии (SVR) авторы предлагают способ формирования новых признаков на основе сгенерированных отступов (\emph{margins}) каждым классификатором, приводят алгоритм обучения на полученных объектах и анализируют отличия множеств опорных объектов для различных ядер. В качестве практической проверки были проведены эксперименты на различных реальных данных из репозитория UCI.

    \textbf{Ключевые слова:} SVM combination, kernel-trick, агрегирование алгоритмов, синергия}
    
        \noindent\makebox[\linewidth]{\rule{\textwidth}{0.4pt}}

    \vspace{1cm}
    
    \begin{multicols}{2}

\section*{Введение}
%	\emph{Основное сообщение — чему посвящена работа (одна-две фразы)}
	
	Работа посвящена комбинированию небольшого количества сильных SVM, использующих kernel-trick с различными ядрами и получению агрегированного классификатора для улучшения решений задач классификации и регрессии. 
	
%	\emph{Обзор литературы — развитие предлагаемой идеи (не более двух абзацев)}
	
	SVM(Support Vector Machine) или \emph{метод опорных векторов}\cite{Vapnik1998}\cite{Cortes1995} \cite{Boser1992} ---~это один из наиболее распространенных и эффективных методов в машинном обучении. Основными достоинствами метода являются: 
		\begin{itemize}
			\item Эффективность численных методов решения
			\item Обозначение опорных объектов из обучающих выборок
			\item Обобщение на нелинейные классификаторы (kernel-trick)
		\end{itemize}
	SVM используются для задач классификации и регрессии (SVR). Задача математического программирования сводится к двойственной задаче, функционалы в которой не зависят от векторов признаков как таковых, а лишь от их попарных скалярных произведений \cite{Voron} . Использование особых функций, \emph{ядер}, то есть скалярных произведений в сопряженном пространстве, позволяет получить разделяющие поверхности между классами более сложной формы \cite{Smola2004}. Наша цель --- скомбинировать SVM
	с различными примененными ядрами для улучшения решения, а также анализ множеств опорных объектов в случае разных использованных ядер.
	
%	\emph{Современное состояние области (два-четыре абзаца)}
	
	Наиболее классическими методами агрегирования алгоритмов являются 
	бэггинг (\emph{bagging})\cite{Breiman1996} и бустинг (\emph{boosting}) \cite{Freund1995}, и их
	вариации, однако они работают только с  большим количеством слабых классификаторов, что делает невозможным использование его использование для указанного множества базовых алгоритмов.
	
	Среди способов агрегации для небольшого количества классификаторов можно
	выделить, например, выбор большинства классификаторов \cite{Franke1992}, 
	комбинирование ранжирований (rankings) по классам, сделанных различными
	классификаторами \cite{Ho1994}. В дальнейшем было показано, что все подобные 
	методы есть особые случаи составного классификатора из \cite{Kittler1996}, 
	появляющиеся при особых условиях или способах аппроксимации.
	
	Различные способы агрегации SVМ используются во многих задачах анализа данных. 
	\Citeauthor{Martin-merino2007} использовали совокупность SVМ для уменьшения ошибочно негативных классификаций (FP) в задаче фильтрации спама среди электронных писем. 
	Для этого на электронных письмах были введены различные метрики, для каждой из них был приспособлен SVM, а затем результат получался голосованием \cite{Kittler1996}. 
	\Citeauthor{Gorgevik2005}, решавшие задачу распознавания написанных рукой символов, делили множество признаков на четыре непересекающихся подмножества, и на каждом из них обучали SVM, увеличив этим самым коэффициент распознавания по сравнению с одним SVM.
	
	В последнее время стал набирать популярность \emph{метод многоядерного обучения} (MKL, multiple kernel learning) \cite{Dyrba2015} \cite{Bucak2014}\cite{Althloothi2014}, который основывается на том, что линейная комбинация ядер также является ядром. Данный метод хорош при объединении данных из нескольких источников и полной автоматизации, так как суперпозиция функций может быть оптимизирована любым методом валидации (например кросс-валидацией).
	 
%	\emph{Что предлагается (два абзаца)}
	
	Мы также предлагаем использовать накопившийся банк ядер, однако не на этапе обучения SVM, а на этапе агрегирования обученных алгоритмов. Известно, что алгоритм $b_i$ для объекта $x_j$ обучающей выборки генерирует \emph{отступ} (margin). По отступу в общем случае можно определить не только предсказанный класс, но и насколько <<уверен>> в своем решении алгоритм. В случае банка с $n$ ядрами и обучающей выборки с $m$ сэмплами
	мы получим матрицу отступов $M \in \R^{m\times n}$. Отнормировав ее, мы получим новую матрицу <<объект-признак>>, где вектором признаков каждого объекта будет вектор отнормированных отступов.
	
	В этой работе предложен алгоритм обучения на матрице отступов, проведен анализ опорных объектов, генерируемые различными ядрами, а также проведено тестирование полученного алгоритма на реальных данных репозитория UCI. 

\nocite{*}
\printbibliography
\end{multicols}


\end{document}

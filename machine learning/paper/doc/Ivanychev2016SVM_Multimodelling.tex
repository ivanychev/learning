\documentclass[12pt,twoside]{article}
\usepackage{jmlda}
\usepackage{hyperref}       % clickable links
\usepackage{Iv_commands}    % you can delete this

\newcommand{\hdir}{.}
\renewcommand{\L}{\mathcal{L}}
\newcommand{\lto}{\leftarrow}



\theoremstyle{plain}
\newtheorem{thm}{Теорема}[section]
\newtheorem{lem}{Лемма}[section]

\theoremstyle{remark}
\newtheorem{rem}{Ремарка}[section]

\theoremstyle{definition}
\newtheorem{df}{Определение}[section]

% version 0.4

\title
    {Синергия алгоритмов классификации (SVM Multimodelling)}
\author
    {С.\,~Иванычев, А.\,~Адуенко}
\email
    {\href{mailto:sergeyivanychev@gmail.com}{sergeyivanychev@gmail.com},  \href{mailto:aduenko1@gmail.com}{aduenko1@gmail.com}}
\organization
    {Московский физико-технический институт}
\abstract
    {В данной статье рассматривается проблема агрегирования небольшого количества сильных классификаторов с целью улучшения решений задач классификации и регрессии.  В качестве примера подобной системы рассматривается система SVM  алгоритмов использующая kernel-trick с различными ядрами. Для комбинации решений и улучшения качества прогнозирования в задачах классификации и регрессии (SVR) авторы предлагают способ формирования новых признаков на основе сгенерированных отступов (\emph{margins}) каждым классификатором, приводят алгоритм обучения на полученных объектах и анализируют отличия множеств опорных объектов для различных ядер. В качестве практической проверки были проведены эксперименты на различных реальных данных из репозитория UCI.

    \bigskip
\noindent
\textbf{Ключевые слова}: \emph {двухклассовая классификация, композиция алгоритмов, SVM, SVR, бэггинг}
}

\bibliographystyle{unsrt}

\begin{document}
\maketitle

\section{Введение}
Работа посвящена комбинированию небольшого количества сильных SVM, использующих kernel-trick с различными ядрами и получению агрегированного классификатора для улучшения решений задач классификации и регрессии.

    SVM(Support Vector Machine) или \emph{метод опорных векторов}\cite{Cortes1995, Boser1992} ---~это один из наиболее распространенных и эффективных методов в машинном обучении, которые используется для задач классификации и регрессии (SVR). Задача математического программирования сводится к двойственной задаче, функционалы в которой не зависят от векторов признаков как таковых, а лишь от их попарных скалярных произведений. Использование особых функций, \emph{ядер}, то есть скалярных произведений в сопряженном пространстве, позволяет получить разделяющие поверхности между классами более сложной формы \cite{Smola2004}. Наша цель --- скомбинировать SVM
    с различными примененными ядрами для улучшения решения, а также анализ множеств опорных объектов в случае разных использованных ядер.

    Наиболее классическими методами агрегирования алгоритмов являются
    бэггинг (\emph{bagging})\cite{Breiman1996} и бустинг (\emph{boosting}) \cite{Freund1995}, и их
    вариации, однако они работают только с  большим количеством слабых классификаторов, что делает невозможным использование его использование для указанного множества базовых алгоритмов.

    Среди способов агрегации для небольшого количества классификаторов можно
    выделить, например, выбор большинства классификаторов \cite{Franke1992},
    комбинирование ранжирований (rankings) по классам, сделанных различными
    классификаторами \cite{Ho1994}. В дальнейшем было показано, что все подобные
    методы есть особые случаи составного классификатора из \cite{Kittler1996},
    появляющиеся при особых условиях или способах аппроксимации.

    Различные способы агрегации SVМ используются во многих задачах анализа данных.
    \cite{Martin-merino2007} использовали совокупность SVМ для уменьшения ошибочно негативных классификаций (FP) в задаче фильтрации спама среди электронных писем.
    Для этого на электронных письмах были введены различные метрики, для каждой из них был приспособлен SVM, а затем результат получался голосованием \cite{Kittler1996}.
    \cite{Gorgevik2005}, решавшие задачу распознавания написанных рукой символов, делили множество признаков на четыре непересекающихся подмножества, и на каждом из них обучали SVM, увеличив этим самым коэффициент распознавания по сравнению с одним SVM.

    В последнее время стал набирать популярность \emph{метод многоядерного обучения} (MKL,~multiple kernel learning) \cite{Dyrba2015, Bucak2014,Althloothi2014}, который основывается на том, что линейная комбинация ядер также является ядром. Данный метод хорош при объединении данных из нескольких источников и полной автоматизации, так как суперпозиция функций может быть оптимизирована любым методом валидации (например кросс-валидацией).

    Мы также предлагаем использовать накопившийся банк ядер, однако не на этапе обучения SVM, а на этапе агрегирования обученных алгоритмов. Известно, что алгоритм $b_i$ для объекта $x_j$ обучающей выборки генерирует \emph{отступ} (margin). По отступу в общем случае можно определить не только предсказанный класс, но и насколько <<уверен>> в своем решении алгоритм. В случае банка с $n$ ядрами и обучающей выборки с $m$ сэмплами
    мы получим матрицу отступов $M \in \R^{m\times n}$. Отнормировав ее, мы получим новую матрицу <<объект-признак>>, где вектором признаков каждого объекта будет вектор отнормированных отступов.

    В этой работе предложен алгоритм обучения на матрице отступов, проведен анализ опорных объектов, генерируемые различными ядрами, а также проведено тестирование полученного алгоритма на реальных данных репозитория UCI.


\section{Постановка задачи}

Пусть $X^l = \brs{\vec{x}_i, y_i}_{i=1}^l$~---~обучающая выборка, $\vec{x}\in \R^n, y \in \{\pm 1\}$.

\begin{df}
	Под $s$-й \emph{моделью} будем понимать SVM с
ядром $K_s$ где выбрано множество ядер:
$$
\mathcal{K} = \{K_i\}_{j=1}^m
$$
\end{df}
	
При обучении каждая модель дает классификатор или регрессор (в зависимости от типа $Y$). Например, для случая $Y \in \{-1, +1\}$ классификации алгоритм выглядит следующим образом:

$$
b_s(\v{x}) = \sign{\sum_{i=1}^l\lambda_i y_i K_s(\vec{x}_i, \vec{x}) - w_0}
$$

Где $\{\lambda_i\}$ и $w_0$ находятся из решения задачи математического программирования\cite{Smola2004}

\begin{equation*}
 \begin{cases}
   \sum_{i=1}^l \lambda_i + \frac{1}{2}\sum_{i=1}^{l}\sum_{j=1}^{l}
    \lambda_i \lambda_j y_i y_j K_s(\vec{x}_i, \vec{x}_j) \to \min_\lambda
   \\
   0 \leq \lambda_s \leq c, \;\; i = 1\ldots l
   \\
   \sum_{i=1}^l\lambda_i y_i = 0
 \end{cases}
\end{equation*}

Результатом обучения является то, что модель для обучающей выборки генерирует вектор \emph{отступов} (margins) для каждого объекта. Совокупность отступов порождает матрицу отступов  размерности
$M \in \R^{l\times m}$,
в котором $(i, j)$-й элемент ---~это отступ $i$-го объекта в SVM с~$j$-м ядром. Авторы утверждают, что эта матрица может быть использована как новая обучающая выборка для построения агрегирующего алгоритма.

Рассмотрим $M$ как новую матрицу <<объект-признак>>, $\mathcal{A}$ ---~ множество алгоритмов
вида
\begin{equation}
    \mathcal{A} = \fbrs{a(\vec{x}) = g(\vec{x}, \theta)|\theta \in \Theta}\;\; g:\R^m \to Y
\end{equation}
где $\Theta$ --- некое семейство параметров.
\begin{df}
Пару $(g, \mathcal{K})$ назовем \emph{мультимоделью}.
\end{df}

$L(y, y^*)$ ---~функционал качества. Тогда перед нами стоит 
Задача выбора алгоритма агрегирования сильных классификаторов супермодели:
	\begin{equation}
    	L(y, g(M(X), \theta)) \to \min_{\Theta}
	\end{equation}

В качестве этой функции мы предлагаем использовать площадь под ROC-кривой (AUC-ROC) как устойчивый к несбалансированностям в выборке.
\section{Близость моделей}

Так как агрегация сильных классификаторов должна быть устойчива к похожим моделям в ее составе, она должна каким-то образом определять идентичные SVM. В данной секции попробуем установить связь между корреляцией векторов отступов,
генерируемых
разными ядрами на одной и той же обучающей выборке и степенью <<похожести>>
ядер. Понятие <<похожести>> или близости отождествляется с введением метрики
на пространстве ядер. 
\begin{df}
	\emph{Расстоянием между ядрами} $\rho(K_i, K_j)$ на выборке $X^l$ будем называть функцию:
$$
\rho_{X^l}(K_i, K_j) = \frac{\#\sbrs{\mathrm{SV}_i \;\Delta\; \mathrm{SV}_j}}
{\#\sbrs{\mathrm{SV}_i \cup \mathrm{SV}_j}}
$$
Где под $\mathrm{SV}_j$ понимается множество опорных объектов на $j$-м ядре, под
знаком $\Delta$ --- симметрическая разность.
\end{df}
\begin{df}
	\emph{Похожестью SVM с ядрами} $K_i, K_j$ будем называть следующую функцию
$$
s_{X^l}(K_i, K_j) = 1 - \mathrm{corr}(M_i, M_j)
$$	
где под $M_i$ понимается вектор отступов соответственного SVM. 
\end{df}

В качестве исходных данных
возьмем датасеты Wine \cite{UCI:Wine}, German credit data\cite{UCI:German} и
Heart disease\cite{UCI:Heart}. Варьируя коэффициент L2-регуляризации, в качестве базового набора ядер возьмем

\begin{itemize}
    \item Линейное
    \item Полиномиальное (степени 3, 4, 5)
    \item RBF-ядро ($\gamma \in \{0.0001, 0.001, 0.01, 0.1, 1\}$)
\end{itemize}

Для каждого значения коэффициента регуляризации построим графики
опишем полученное множество классификаторов.

% Выглядит очень разумно. Напиши какие-то выводы.
% Опиши, для какого датасета 
% - сколько векторов, 
% - насколько в среднем похожи, 
% - насколько результаты для разных ядер похожи, 
% - насколько похожи их множества опорных объектов.

% Опиши, как влияет параметр С, сделай глобальный вывод применительно ко всем датасетам, какие пары ядер дают схожие результаты.
% Укажи на различия, где различия в векторах напрямую дают различие в ответах, а где нет и в попробуй определить, почему.

\newpage

\subsection{German credit dataset}

\begin{figure}[H]
      \center{\includegraphics[width=\textwidth]{german.pdf}}
      \caption{German credit}
\end{figure}

\input{german.tex_table}


Обучающая выборка состоит из 1000 семплов по 24 числовых признака. 

\newpage

\subsection{Wine dataset}


\begin{figure}[H]
      \center{\includegraphics[width=\textwidth]{wine.pdf}}
      \caption{Wine quality}
\end{figure}

\input{wine.tex_table}


\newpage

\subsection{Heart dataset}

\begin{figure}[H]
      \center{\includegraphics[width=\textwidth]{heart.pdf}}
      \caption{Heart disease}
\end{figure}

\input{heart.tex_table}

\subsection{Выводы из эксперимента}

Уже по этим данным можно сказать, что (будем называть $1 - \mathrm{corr}(M_i, M_j)$ 
\emph{расстоянием между отступами}).
\begin{itemize}
  \item С ростом константы регуляризации расстояние между ядрами и расстояние между	
  их отступами лучше коррелируют между собой.
  \item При высоких параметре регуляризации коэффициент корреляции Пирсона 
  достигает более $0.8$, то есть расстояния практически линейно зависят друг от друга.
  \item Вектора средних ядерных и отступных расстояний кореллируют по-разному на различных датасетах (на Wine и Heart корелляции Пирсона $0.85$ и $0.99$ соответственно, на German --- $-0.92$).
\end{itemize} 

\section{Описание алгоритма}

\subsection{Модели}

В качестве множества ядер, участвующих в мультимодели выберем следующий набор

\begin{itemize}
    \item Линейное
    \item Полиномиальное (степени 3, 4, 5)
    \item RBF-ядро ($\gamma \in \{0.0001, 0.001, 0.01, 0.1, 1\}$)
    \item INK-spline ядро (степени 1, 2)
\end{itemize}

Данное множество выбрано как наиболее универсальные представители различных классов ядер. 

\subsection{Мультимодель}

В качестве базового алгоритма агрегации классификаторов выбрана логистическая регрессия.

\newpage

\subsection{Алгоритм}

\begin{algorithm}[!h]
	\caption{Обучение мультимодели SVM в случае двухклассовой классификации}
	\label{alg:svm}
	\begin{algorithmic}
	\REQUIRE обучающая выборка $X^l = (\v{X}, y) = (\v{x_i}, y_i)_{i=1}^l, x_i \in R^m, y_i \in \{0, 1\}$, множество ядер $\mathcal{K} = \{K_i\}_{i=1}^n$
	\ENSURE весовые параметры логистической регрессии $w'$, вектор обученных классификаторов $CLS$, нормирующие выборку и отступы коэффициенты
	$\v{w}$;
	\BEGIN
		\STATE отнормировать обучающую выборку 
		$$
		Mean \lto \mathrm{mean}(\v{X}))\;\; Std \lto \mathrm{std}(\v{X})
		$$
		$$
		\v{X} \lto \frac{\v{X} - Mean}{Std}
		$$
		\STATE инициализировать массив классификаторов $CLS$
		\STATE инициализировать матрицу $Margins \in \R^{l\times n}$
		\FOR{$K$ in $\mathcal{K}$}
			\STATE Вычисляем матрицу Грамма $\Gamma_K$ ядра $K$ для объектов $\v{X}$.
			\STATE Обучаем SVM с ядром $K$, решаем оптимизационную задачу
			$$
			\sum_{i=1}^l (1 - M_i(w, w_0))_+ + \frac{1}{2C}||w||^2 \to \min_{w, w_0}
			$$
			Где $M_i(w, w_0) = y_i(K(w, x_i) - w_0)$
			\STATE Добавляем обученный классификатор в вектор $CLS$.
			\STATE $Margins[:, K] \lto \v{M_K} = (M_1\ldots M_l)'$
		\ENDFOR
		\STATE Нормируем отступы, чтобы средний модуль отступов был равен единице 
			$$
			MeanMargins \lto \frac{Margins}{\mathrm{mean}(\mathrm{abs}(Margins))}
			$$
			$$
			Margins \lto \frac{Margins}{MeanMargins}
			$$
		\STATE Обучаем логистическую регрессию на $X^l_* = (Margins, y)$. Решаем задачу минимизации
		
		$$
		Q(w) = \sum_{i=1}^l \log(1 + \mathrm{exp}(-\rbrs{w, x_i}y_i)) \to \min_w\;\;x_i \in Margins
		$$
		$$
		w' \lto \argmin{Q(w)}
		$$
		\RETURN $w'$, $MeanMargins$, $Mean$, $Std$
	\end{algorithmic}
	\end{algorithm}
\begin{algorithm}[!h]
	\caption{Классификация новых объектов обученной мультимоделью SVM в случае двухклассовой классификации}
	\label{alg:svm}
	\begin{algorithmic}
	\REQUIRE тестовая выборка $X^l = (\v{X}, y) = (\v{x_i}, y_i)_{i=1}^l, x_i \in R^m, y_i \in \{0, 1\}$, множество ядер $\mathcal{K} = \{K_i\}_{i=1}^n$, вектор весов $w'$, вектор нормировки $MeanMargins$, $Mean$, $Std$
	\ENSURE вектор спрогнозированных меток классов $y_{pred}$
	\BEGIN
		\STATE отнормировать тестовую выборку 
		$$
		\v{X} \lto \frac{\v{X} - Mean}{Std}
		$$
		\STATE посчитать отступы на поступивших объектах и отнормировать их
		$$
		Margins \lto CLS(X) / MeanMargins
		$$
		\STATE Классифицируем объекты логистической регрессией с вектором весов $w'$
		$$
		y_{pred} \lto LogisticRegression(Margins, w')
		$$
		\RETURN $y_{pred}$
	\end{algorithmic}
	\end{algorithm}

\newpage


\bibliography{papers}

\end{document}

% $ biblatex auxiliary file $
% $ biblatex bbl format version 2.6 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated by
% biber as required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup


\refsection{0}
  \sortlist[entry]{nty/global}
    \entry{UCI:Wine}{misc}{}
      \name{author}{1}{}{%
        {{hash=65b4e4b0599a4094529905e20531c834}{%
           family={Aeberhard},
           family_i={A\bibinitperiod},
           given={Stefan},
           given_i={S\bibinitperiod}}}%
      }
      \list{institution}{3}{%
        {Institute of Pharmaceutical}%
        {Food Analysis}%
        {Technologies}%
      }
      \strng{namehash}{65b4e4b0599a4094529905e20531c834}
      \strng{fullhash}{65b4e4b0599a4094529905e20531c834}
      \field{sortinit}{A}
      \field{sortinithash}{b685c7856330eaee22789815b49de9bb}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{Wine Data Set}
      \field{year}{1991}
      \verb{url}
      \verb https://archive.ics.uci.edu/ml/datasets/Wine
      \endverb
    \endentry
    \entry{Althloothi2014}{article}{}
      \name{author}{4}{}{%
        {{hash=bad1e320121c2bf1d121691d8ed4d0b6}{%
           family={Althloothi},
           family_i={A\bibinitperiod},
           given={Salah},
           given_i={S\bibinitperiod}}}%
        {{hash=796e9202f423522c4410a7ac8f37fbc9}{%
           family={Mahoor},
           family_i={M\bibinitperiod},
           given={Mohammad\bibnamedelima H.},
           given_i={M\bibinitperiod\bibinitdelim H\bibinitperiod}}}%
        {{hash=227e34554b27c77e2d042504ef1edb97}{%
           family={Zhang},
           family_i={Z\bibinitperiod},
           given={Xiao},
           given_i={X\bibinitperiod}}}%
        {{hash=72e047739700978a1a104760e36716fa}{%
           family={Voyles},
           family_i={V\bibinitperiod},
           given={Richard\bibnamedelima M.},
           given_i={R\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Elsevier}%
      }
      \strng{namehash}{6e7545b9daa479ecc6254732d2cbfbbb}
      \strng{fullhash}{0782dee2b3b7a5ed1925a8a19cb35d5b}
      \field{sortinit}{A}
      \field{sortinithash}{b685c7856330eaee22789815b49de9bb}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This paper presents two sets of features, shape representation and kinematic structure, for human activity recognition using a sequence of RGB-D images. The shape features are extracted using the depth information in the frequency domain via spherical harmonics representation. The other features include the motion of the 3D joint positions (i.e. the end points of the distal limb segments) in the human body. Both sets of features are fused using the Multiple Kernel Learning (MKL) technique at the kernel level for human activity recognition. Our experiments on three publicly available datasets demonstrate that the proposed features are robust for human activity recognition and particularly when there are similarities among the actions. ?? 2013 Elsevier Ltd. All rights reserved.}
      \field{issn}{00313203}
      \field{journaltitle}{Pattern Recognition}
      \field{number}{5}
      \field{title}{{Human activity recognition using multi-features and multiple kernel learning}}
      \field{volume}{47}
      \field{year}{2014}
      \field{pages}{1800\bibrangedash 1812}
      \range{pages}{13}
      \verb{doi}
      \verb 10.1016/j.patcog.2013.11.032
      \endverb
      \verb{file}
      \verb :Users/sergeyivanychev/Library/Application Support/Mendeley Desktop/Downloaded/Althloothi et al. - 2014 - Human activity recognition using multi-features and multiple kernel learning.pdf:pdf
      \endverb
      \verb{url}
      \verb http://dx.doi.org/10.1016/j.patcog.2013.11.032
      \endverb
      \keyw{Distal limb segments,Human activity recognition,Multiple kernel learning,Spherical harmonics coefficients,Support vector machines}
    \endentry
    \entry{Boser1992}{article}{}
      \name{author}{3}{}{%
        {{hash=a012d21d84fb6ec87416be7815d7fbd9}{%
           family={Boser},
           family_i={B\bibinitperiod},
           given={Bernhard\bibnamedelima E.},
           given_i={B\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
        {{hash=495d7b70e4ce0aae74ceb20f9a611ed1}{%
           family={Guyon},
           family_i={G\bibinitperiod},
           given={Isabelle\bibnamedelima M.},
           given_i={I\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
        {{hash=196dca851d60592649a7d411a61357a1}{%
           family={Vapnik},
           family_i={V\bibinitperiod},
           given={Vladimir\bibnamedelima N.},
           given_i={V\bibinitperiod\bibinitdelim N\bibinitperiod}}}%
      }
      \strng{namehash}{f442a8df2270d920afa6e9741ef1627e}
      \strng{fullhash}{f442a8df2270d920afa6e9741ef1627e}
      \field{sortinit}{B}
      \field{sortinithash}{4ecbea03efd0532989d3836d1a048c32}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented. The technique is applicable to a wide variety of classifiaction functions, including Perceptrons, polynomials, and Radial Basis Functions. The effective number of parameters is adjusted automatically to match the complexity of the problem. The solution is expressed as a linear combination of supporting patterns. These are the subset of training patterns that are closest to the decision boundary. Bounds on the generalization performance based on the leave-one-out method and the VC-dimension are given. Experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning algorithms. 1 INTRODUCTION Good generalization performance of pattern classifiers is achieved when the capacity of the classification function is matched to the size of the training set. Classifiers with a large numb...}
      \field{eprinttype}{arXiv}
      \field{isbn}{089791497X}
      \field{issn}{0-89791-497-X}
      \field{journaltitle}{Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory}
      \field{title}{{A Training Algorithm for Optimal Margin Classifiers}}
      \field{year}{1992}
      \field{pages}{144\bibrangedash 152}
      \range{pages}{9}
      \verb{doi}
      \verb 10.1.1.21.3818
      \endverb
      \verb{eprint}
      \verb arXiv:1011.1669v3
      \endverb
      \verb{file}
      \verb :Users/sergeyivanychev/Library/Application Support/Mendeley Desktop/Downloaded/Boser, Guyon, Vapnik - 1992 - A Training Algorithm for Optimal Margin Classifiers.pdf:pdf
      \endverb
      \verb{url}
      \verb http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.21.3818
      \endverb
    \endentry
    \entry{Breiman1996}{article}{}
      \name{author}{1}{}{%
        {{hash=132b7100417675d55d5d4d8b244f7a34}{%
           family={Breiman},
           family_i={B\bibinitperiod},
           given={Leo},
           given_i={L\bibinitperiod}}}%
      }
      \strng{namehash}{132b7100417675d55d5d4d8b244f7a34}
      \strng{fullhash}{132b7100417675d55d5d4d8b244f7a34}
      \field{sortinit}{B}
      \field{sortinithash}{4ecbea03efd0532989d3836d1a048c32}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Bagging predictors is a method for generating multiple versions of a pre-dictor and using these to get an aggregated predictor. The aggregation av-erages over the versions when predicting a numerical outcome and does a plurality v ote when predicting a class. The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. Tests on real and simulated data sets using classiication and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy. The vital element is the instability o f the prediction method. If perturbing the learning set can cause signiicant changes in the predictor constructed, then bagging can improve accuracy.}
      \field{isbn}{0885-6125}
      \field{issn}{0885-6125}
      \field{journaltitle}{Machine Learning}
      \field{number}{421}
      \field{title}{{Bagging Predictors}}
      \field{volume}{24}
      \field{year}{1996}
      \field{pages}{123\bibrangedash 140}
      \range{pages}{18}
      \verb{doi}
      \verb 10.1007/BF00058655
      \endverb
      \verb{file}
      \verb :Users/sergeyivanychev/Library/Application Support/Mendeley Desktop/Downloaded/Breiman - 1996 - Bagging Predictors.pdf:pdf
      \endverb
      \keyw{aggregation,averaging,bootstrap,combining}
    \endentry
    \entry{Bucak2014}{article}{}
      \name{author}{3}{}{%
        {{hash=6935407c1a6af22ea5d0ca784a1ee245}{%
           family={Bucak},
           family_i={B\bibinitperiod},
           given={S.S.},
           given_i={S\bibinitperiod}}}%
        {{hash=ba1406d04c6a736613cef3a57477db03}{%
           family={Jin},
           family_i={J\bibinitperiod},
           given={R.},
           given_i={R\bibinitperiod}}}%
        {{hash=cedbb638d990c20ddad721dd25a57829}{%
           family={Jain},
           family_i={J\bibinitperiod},
           given={Ak.},
           given_i={A\bibinitperiod}}}%
      }
      \strng{namehash}{2d9bd660e3569a8788d6926f9ba7b551}
      \strng{fullhash}{2d9bd660e3569a8788d6926f9ba7b551}
      \field{sortinit}{B}
      \field{sortinithash}{4ecbea03efd0532989d3836d1a048c32}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Multiple kernel learning (MKL) is a principled approach for selecting and combining kernels for a given recognition task. A number of studies have shown that MKL is a useful tool for object recognition, where each image is represented by multiple sets of features and MKL is applied to combine different feature sets. We review the state-of-the-art for MKL, including different formulations and algorithms for solving the related optimization problems, with the focus on their applications to object recognition. One dilemma faced by practitioners interested in using MKL for object recognition is that different studies often provide conflicting results about the effectiveness and efficiency of MKL. To resolve this, we conduct extensive experiments on standard datasets to evaluate various approaches to MKL for object recognition. We argue that the seemingly contradictory conclusions offered by studies are due to different experimental setups. The conclusions of our study are: (i) given a sufficient number of training examples and feature/kernel types, MKL is more effective for object recognition than simple kernel combination (e.g., choosing the best performing kernel or average of kernels); and (ii) among the various approaches proposed for MKL, the sequential minimal optimization, semi-infinite programming, and level method based ones are computationally most efficient.}
      \field{isbn}{0162-8828 VO - 36}
      \field{issn}{0162-8828}
      \field{journaltitle}{IEEE Transactions on Pattern Analysis and Machine Intelligence}
      \field{number}{7}
      \field{title}{{Multiple Kernel Learning for Visual Object Recognition: A Review}}
      \field{volume}{36}
      \field{year}{2014}
      \field{pages}{1354\bibrangedash 1369}
      \range{pages}{16}
      \verb{doi}
      \verb 10.1109/TPAMI.2013.212
      \endverb
      \verb{file}
      \verb :Users/sergeyivanychev/Library/Application Support/Mendeley Desktop/Downloaded/Bucak, Jin, Jain - 2014 - Multiple Kernel Learning for Visual Object Recognition A Review.pdf:pdf
      \endverb
      \verb{url}
      \verb http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6654166
      \endverb
      \keyw{Feature evaluation and selection,Histograms,Image color analysis,Introductory and Survey,Kernel,Machine learning,Multiple kernel learning,Object recognition,Optimization,Training,Visualization,convex optimization,support vector machine,visual object recognition}
    \endentry
    \entry{Cortes1995}{article}{}
      \name{author}{2}{}{%
        {{hash=17acda211a651e90e228f1776ee07818}{%
           family={Cortes},
           family_i={C\bibinitperiod},
           given={Corinna},
           given_i={C\bibinitperiod}}}%
        {{hash=c2b3e05872463585b4be6aab10d10d63}{%
           family={Vapnik},
           family_i={V\bibinitperiod},
           given={Vladimir},
           given_i={V\bibinitperiod}}}%
      }
      \strng{namehash}{4c67d5268f413e83454c8adc14ab43c3}
      \strng{fullhash}{4c67d5268f413e83454c8adc14ab43c3}
      \field{sortinit}{C}
      \field{sortinithash}{59f25d509f3381b07695554a9f35ecb2}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Oil/water partition coefficient (log P) is one of the key points for lead compound to be drug. In silico log P models based solely on chemical structures have become an important part of modern drug discovery. Here, we report support vector machines, radial basis function neural networks, and multiple linear regression methods to investigate the correlation between partition coefficient and physico-chemical descriptors for a large data set of compounds. The correlation coefficient r (2) between experimental and predicted log P for training and test sets by support vector machines, radial basis function neural networks, and multiple linear regression is 0.92, 0.90, and 0.88, respectively. The results show that non-linear support vector machines derives statistical models that have better prediction ability than those of radial basis function neural networks and multiple linear regression methods. This indicates that support vector machines can be used as an alternative modeling tool for quantitative structure-property/activity relationships studies.}
      \field{eprinttype}{arXiv}
      \field{isbn}{0885-6125}
      \field{issn}{15730565}
      \field{journaltitle}{Machine Learning}
      \field{number}{3}
      \field{title}{{Support-Vector Networks}}
      \field{volume}{20}
      \field{year}{1995}
      \field{pages}{273\bibrangedash 297}
      \range{pages}{25}
      \verb{doi}
      \verb 10.1023/A:1022627411411
      \endverb
      \verb{eprint}
      \verb arXiv:1011.1669v3
      \endverb
      \keyw{efficient learning algorithms,neural networks,pattern recognition,polynomial classifiers,radial basis function classifiers}
    \endentry
    \entry{Dyrba2015}{article}{}
      \name{author}{4}{}{%
        {{hash=f0ecfaf7b60a4e07e390e6a0535e3158}{%
           family={Dyrba},
           family_i={D\bibinitperiod},
           given={Martin},
           given_i={M\bibinitperiod}}}%
        {{hash=6eddd86ce3dedbcbd3659482e7fa876e}{%
           family={Grothe},
           family_i={G\bibinitperiod},
           given={Michel},
           given_i={M\bibinitperiod}}}%
        {{hash=a4b41b19352cd7c9b7ef54a21d69e850}{%
           family={Kirste},
           family_i={K\bibinitperiod},
           given={Thomas},
           given_i={T\bibinitperiod}}}%
        {{hash=45e0424bf8d706d09330b9472d11088d}{%
           family={Teipel},
           family_i={T\bibinitperiod},
           given={Stefan\bibnamedelima J.},
           given_i={S\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
      }
      \strng{namehash}{5b92073cb25a1145901594680106c9c3}
      \strng{fullhash}{1ff840bc72d26e6d38356d11d72ba6e8}
      \field{sortinit}{D}
      \field{sortinithash}{78f7c4753a2004675f316a80bdb31742}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Alzheimer's disease (AD) patients exhibit alterations in the functional connectivity between spatially segregated brain regions which may be related to both local gray matter (GM) atrophy as well as a decline in the fiber integrity of the underlying white matter tracts. Machine learning algorithms are able to automatically detect the patterns of the disease in image data, and therefore, constitute a suitable basis for automated image diagnostic systems. The question of which magnetic resonance imaging (MRI) modalities are most useful in a clinical context is as yet unresolved. We examined multimodal MRI data acquired from 28 subjects with clinically probable AD and 25 healthy controls. Specifically, we used fiber tract integrity as measured by diffusion tensor imaging (DTI), GM volume derived from structural MRI, and the graph-theoretical measures 'local clustering coefficient' and 'shortest path length' derived from resting-state functional MRI (rs-fMRI) to evaluate the utility of the three imaging methods in automated multimodal image diagnostics, to assess their individual performance, and the level of concordance between them. We ran the support vector machine (SVM) algorithm and validated the results using leave-one-out cross-validation. For the single imaging modalities, we obtained an area under the curve (AUC) of 80{\%} for rs-fMRI, 87{\%} for DTI, and 86{\%} for GM volume. When it came to the multimodal SVM, we obtained an AUC of 82{\%} using all three modalities, and 89{\%} using only DTI measures and GM volume. Combined multimodal imaging data did not significantly improve classification accuracy compared to the best single measures alone. Hum Brain Mapp 36:2118-2131, 2015. © 2015 Wiley Periodicals, Inc.}
      \field{issn}{10970193}
      \field{journaltitle}{Human Brain Mapping}
      \field{number}{6}
      \field{title}{{Multimodal analysis of functional and structural disconnection in Alzheimer's disease using multiple kernel SVM}}
      \field{volume}{36}
      \field{year}{2015}
      \field{pages}{2118\bibrangedash 2131}
      \range{pages}{14}
      \verb{doi}
      \verb 10.1002/hbm.22759
      \endverb
      \verb{file}
      \verb :Users/sergeyivanychev/Library/Application Support/Mendeley Desktop/Downloaded/Dyrba et al. - 2015 - Multimodal analysis of functional and structural disconnection in Alzheimer's disease using multiple kernel SVM.pdf:pdf
      \endverb
      \keyw{Alzheimer's disease,Diffusion tensor imaging,Magnetic resonance imaging,Multiple kernel support vector machine,Resting-state functional magnetic resonance imagin}
    \endentry
    \entry{Freund1995}{misc}{}
      \name{author}{1}{}{%
        {{hash=7652bc7c7334cc235c609273f6dabeab}{%
           family={Freund},
           family_i={F\bibinitperiod},
           given={Y.},
           given_i={Y\bibinitperiod}}}%
      }
      \strng{namehash}{7652bc7c7334cc235c609273f6dabeab}
      \strng{fullhash}{7652bc7c7334cc235c609273f6dabeab}
      \field{sortinit}{F}
      \field{sortinithash}{c6a7d9913bbd7b20ea954441c0460b78}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We present an algorithm for improving the accuracy of algorithms for learning binary concepts. The improvement is achieved by combining a large number of hypotheses, each of which is generated by training the given learning algorithm on a different set of examples. Our algorithm is based on ideas presented by Schapire and represents an improvement over his results, The analysis of our algorithm provides general upper bounds on the resources required for learning in Valiants polynomial PAC learning framework, which are the best general upper bounds known today. We show that the number of hypotheses that are combined by our algorithm is the smallest number possible. Other outcomes of our analysis are results regarding the representational power of threshold circuits, the relation between learnability and compression, and a method for parallelizing PAC learning algorithms. We provide extensions of our algorithms to cases in which the concepts are not binary and to the case where the accuracy of the learning algorithm depends on the distribution of the instances.}
      \field{booktitle}{Information and Computation}
      \field{isbn}{1558601465}
      \field{issn}{08905401}
      \field{number}{2}
      \field{title}{{Boosting a Weak Learning Algorithm by Majority}}
      \field{volume}{121}
      \field{year}{1995}
      \field{pages}{256\bibrangedash 285}
      \range{pages}{30}
      \verb{doi}
      \verb 10.1006/inco.1995.1136
      \endverb
      \verb{file}
      \verb :Users/sergeyivanychev/Dropbox/Work/6 term/paper/papers/base/boosting.pdf:pdf
      \endverb
      \verb{url}
      \verb http://linkinghub.elsevier.com/retrieve/pii/S0890540185711364
      \endverb
    \endentry
    \entry{Gorgevik2005}{article}{}
      \name{author}{2}{}{%
        {{hash=e7992727478ad2bbed8faf1a46de33e5}{%
           family={Gorgevik},
           family_i={G\bibinitperiod},
           given={D.},
           given_i={D\bibinitperiod}}}%
        {{hash=a38c1d2e1d51449c83379694102544a7}{%
           family={Cakmakov},
           family_i={C\bibinitperiod},
           given={D.},
           given_i={D\bibinitperiod}}}%
      }
      \strng{namehash}{745f1265de844ccc2a30b66ca592be53}
      \strng{fullhash}{745f1265de844ccc2a30b66ca592be53}
      \field{sortinit}{G}
      \field{sortinithash}{1c854ef9177a91bf894e66485bdbd3ed}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Recent results in pattern recognition have shown that SVM (support vector machine) classifiers often have superior recognition rates in comparison to other classification methods. In this paper, a cooperation of four SVM classifiers for handwritten digit recognition, each using different feature set is examined. We investigate the advantages and weaknesses of various cooperation schemes based on classifier decision fusion using statistical reasoning. The obtained results show that it is difficult to exceed the recognition rate of a single, well-tuned SVM classifier applied straightforwardly on all feature sets. In our experiments only one of the cooperation schemes exceeds the recognition rate of a single SVM classifier. However, the classifier cooperation reduces the classifier complexity and need for training samples, decreases classifier training time and sometimes improves the classifier performance}
      \field{isbn}{1-4244-0049-X}
      \field{journaltitle}{EUROCON 2005 - The International Conference on Computer as a Tool""}
      \field{number}{February}
      \field{title}{{Handwritten Digit Recognition by Combining SVM Classifiers}}
      \field{volume}{2}
      \field{year}{2005}
      \field{pages}{1393\bibrangedash 1396}
      \range{pages}{4}
      \verb{doi}
      \verb 10.1109/EURCON.2005.1630221
      \endverb
      \verb{file}
      \verb :Users/sergeyivanychev/Dropbox/Work/6 term/paper/papers/Gorgevik.pdf:pdf
      \endverb
      \keyw{classifier,decision fusion,features,statistical}
    \endentry
    \entry{UCI:Housing}{misc}{}
      \name{author}{2}{}{%
        {{hash=30446210d83b173a79d8fe9c94172a25}{%
           family={Harrison},
           family_i={H\bibinitperiod},
           given={D.},
           given_i={D\bibinitperiod}}}%
        {{hash=490fe0e566a2abd5ccd4ae12f4fcf3f2}{%
           family={Rubinfeld},
           family_i={R\bibinitperiod},
           given={D.L},
           given_i={D\bibinitperiod}}}%
      }
      \strng{namehash}{f60ce27c9e3587a24b7c2a3728c4f284}
      \strng{fullhash}{f60ce27c9e3587a24b7c2a3728c4f284}
      \field{sortinit}{H}
      \field{sortinithash}{82012198d5dfa657b8c4a168793268a6}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{Concerns housing values in suburbs of Boston}
      \field{year}{1993}
      \verb{url}
      \verb https://archive.ics.uci.edu/ml/datasets/Housing
      \endverb
    \endentry
    \entry{UCI:German}{misc}{}
      \name{author}{1}{}{%
        {{hash=14b9929c4659a6922f4fedca42d7221c}{%
           family={Hofmann},
           family_i={H\bibinitperiod},
           given={Dr.\bibnamedelimi Hans},
           given_i={D\bibinitperiod\bibinitdelim H\bibinitperiod}}}%
      }
      \list{institution}{1}{%
        {Institut fur Statistik und Okonometrie}%
      }
      \strng{namehash}{14b9929c4659a6922f4fedca42d7221c}
      \strng{fullhash}{14b9929c4659a6922f4fedca42d7221c}
      \field{sortinit}{H}
      \field{sortinithash}{82012198d5dfa657b8c4a168793268a6}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{Statlog (German Credit Data) Data Set}
      \field{year}{1994}
      \verb{url}
      \verb https://archive.ics.uci.edu/ml/datasets/Statlog+(German+Credit+Data)
      \endverb
    \endentry
    \entry{Izmailov2013}{article}{}
      \name{author}{3}{}{%
        {{hash=56b6a5e0ff1fc4786361f4ee9a2088bb}{%
           family={Izmailov},
           family_i={I\bibinitperiod},
           given={Rauf},
           given_i={R\bibinitperiod}}}%
        {{hash=c2b3e05872463585b4be6aab10d10d63}{%
           family={Vapnik},
           family_i={V\bibinitperiod},
           given={Vladimir},
           given_i={V\bibinitperiod}}}%
        {{hash=d2686633f22ccb74cd4e374c9e291c0c}{%
           family={Vashist},
           family_i={V\bibinitperiod},
           given={Akshay},
           given_i={A\bibinitperiod}}}%
      }
      \strng{namehash}{dd66437410cfce994e78bfa70c29219d}
      \strng{fullhash}{dd66437410cfce994e78bfa70c29219d}
      \field{sortinit}{I}
      \field{sortinithash}{25e99d37ba90f7c4fb20baf4e310faf3}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{isbn}{9781467361293}
      \field{journaltitle}{Proceedings of the International Joint Conference on Neural Networks}
      \field{title}{{Multidimensional splines with infinite number of knots as SVM kernels}}
      \field{volume}{2}
      \field{year}{2013}
      \verb{doi}
      \verb 10.1109/IJCNN.2013.6706860
      \endverb
      \verb{file}
      \verb :Users/iv/Downloads/izmailov2013.pdf:pdf
      \endverb
    \endentry
    \entry{Kittler1996}{article}{}
      \name{author}{3}{}{%
        {{hash=0c733a8ef5c104d05f037b83123f361f}{%
           family={Kittler},
           family_i={K\bibinitperiod},
           given={J.},
           given_i={J\bibinitperiod}}}%
        {{hash=ed9bdce9311c36f9e033c352ed9c6e50}{%
           family={Hater},
           family_i={H\bibinitperiod},
           given={M.},
           given_i={M\bibinitperiod}}}%
        {{hash=9dd467f1ef98d8a9a90c37e12c6d782b}{%
           family={Duin},
           family_i={D\bibinitperiod},
           given={R.\bibnamedelimi P\bibnamedelima W},
           given_i={R\bibinitperiod\bibinitdelim P\bibinitperiod\bibinitdelim W\bibinitperiod}}}%
      }
      \strng{namehash}{38449e102b11ec1f071cb1b3332e9700}
      \strng{fullhash}{38449e102b11ec1f071cb1b3332e9700}
      \field{sortinit}{K}
      \field{sortinithash}{a7d5b3aec5a0890aae7baf85a209abfc}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We develop a common theoretical framework for combining$\backslash$nclassifiers which use distinct pattern representations and show that$\backslash$nmany existing schemes can be considered as special cases of compound$\backslash$nclassification where all the pattern representations are used jointly to$\backslash$nmake a decision. An experimental comparison of various classifier$\backslash$ncombination schemes demonstrates that the combination rule developed$\backslash$nunder the most restrictive assumptions-the sum rule-outperforms other$\backslash$nclassifier combinations schemes. A sensitivity analysis of the various$\backslash$nschemes to estimation errors is carried out to show that this finding$\backslash$ncan be justified theoretically}
      \field{isbn}{081867282X}
      \field{issn}{10514651}
      \field{journaltitle}{Proceedings - International Conference on Pattern Recognition}
      \field{number}{3}
      \field{title}{{Combining classifiers}}
      \field{volume}{2}
      \field{year}{1996}
      \field{pages}{897\bibrangedash 901}
      \range{pages}{5}
      \verb{doi}
      \verb 10.1109/ICPR.1996.547205
      \endverb
      \verb{file}
      \verb :Users/sergeyivanychev/Dropbox/Work/6 term/paper/papers/kittler1998.pdf:pdf
      \endverb
    \endentry
    \entry{Martin-merino2007}{article}{}
      \name{author}{2}{}{%
        {{hash=237ec6fa16e7e45b1f47ab8faccefc9a}{%
           family={Martin-merino},
           family_i={M\bibinithyphendelim m\bibinitperiod},
           given={Manuel},
           given_i={M\bibinitperiod}}}%
        {{hash=20cc6c9abcbbb2167c59e8c95878a86b}{%
           family={Mart},
           family_i={M\bibinitperiod},
           given={Manuel},
           given_i={M\bibinitperiod}}}%
      }
      \strng{namehash}{4576b2d084fb68a65cdcc8d0b3df4bb6}
      \strng{fullhash}{4576b2d084fb68a65cdcc8d0b3df4bb6}
      \field{sortinit}{M}
      \field{sortinithash}{2684bec41e9697b92699b46491061da2}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{isbn}{978-3-540-73006-4}
      \field{number}{February}
      \field{title}{{Combibing SVM Classifiers for Email Anti-spam Filtering}}
      \field{volume}{4507}
      \field{year}{2007}
      \verb{doi}
      \verb 10.1007/978-3-540-73007-1
      \endverb
      \verb{file}
      \verb :Users/sergeyivanychev/Dropbox/Work/6 term/paper/papers/Blanco.pdf:pdf
      \endverb
      \verb{url}
      \verb http://link.springer.com/10.1007/978-3-540-73007-1
      \endverb
    \endentry
    \entry{UCI:Heart}{misc}{}
      \name{author}{4}{}{%
        {{hash=e2a105ba92ca87048e0dc29eba724b3a}{%
           family={M.D},
           family_i={M\bibinitperiod},
           given={Andras\bibnamedelima Janosi},
           given_i={A\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
        {{hash=f9857aec7b57c25c465aa8296c738493}{%
           family={William\bibnamedelima Steinbrunn},
           family_i={W\bibinitperiod\bibinitdelim S\bibinitperiod},
           given={M.D},
           given_i={M\bibinitperiod}}}%
        {{hash=e278ad3807d982ab8e85a531d7b22151}{%
           family={Matthias\bibnamedelima Pfisterer},
           family_i={M\bibinitperiod\bibinitdelim P\bibinitperiod},
           given={M.D},
           given_i={M\bibinitperiod}}}%
        {{hash=f53b4a5512fabae766d1818174eba831}{%
           family={Robert\bibnamedelima Detrano},
           family_i={R\bibinitperiod\bibinitdelim D\bibinitperiod},
           suffix={M.D},
           suffix_i={M\bibinitperiod},
           given={Ph.D},
           given_i={P\bibinitperiod}}}%
      }
      \list{institution}{2}{%
        {Hungarian Institute of Cardiology, Budapest; University Hospital, Zurich; University Hospital, Basel; V.A. Medical Center, Long Beach}%
        {Cleveland Clinic Foundation}%
      }
      \strng{namehash}{9b60e344e565db432ca35fe232549284}
      \strng{fullhash}{80522d1e88a02465bee5ed117495f5bf}
      \field{sortinit}{M}
      \field{sortinithash}{2684bec41e9697b92699b46491061da2}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{Heart Disease Data Set}
      \field{year}{1988}
      \verb{url}
      \verb https://archive.ics.uci.edu/ml/datasets/Heart+Disease
      \endverb
    \endentry
    \entry{scikit-learn}{article}{}
      \name{author}{16}{}{%
        {{hash=22e2e571cc6356b3cb21d40e212ca6ac}{%
           family={Pedregosa},
           family_i={P\bibinitperiod},
           given={F.},
           given_i={F\bibinitperiod}}}%
        {{hash=90a0a2720bfe73efd4029f8a456cdcc3}{%
           family={Varoquaux},
           family_i={V\bibinitperiod},
           given={G.},
           given_i={G\bibinitperiod}}}%
        {{hash=386dd6e7464b390d92ce58cf04ba101c}{%
           family={Gramfort},
           family_i={G\bibinitperiod},
           given={A.},
           given_i={A\bibinitperiod}}}%
        {{hash=ce3bf798cb091d3910827d51160fe0f8}{%
           family={Michel},
           family_i={M\bibinitperiod},
           given={V.},
           given_i={V\bibinitperiod}}}%
        {{hash=b8fa2de348731073d50b89e07267e159}{%
           family={Thirion},
           family_i={T\bibinitperiod},
           given={B.},
           given_i={B\bibinitperiod}}}%
        {{hash=1c2f66c39a6e2b2ca4d545348028a677}{%
           family={Grisel},
           family_i={G\bibinitperiod},
           given={O.},
           given_i={O\bibinitperiod}}}%
        {{hash=f5652799b846fbd40ce19b83c70ef1d3}{%
           family={Blondel},
           family_i={B\bibinitperiod},
           given={M.},
           given_i={M\bibinitperiod}}}%
        {{hash=8d2ca4be32fef4d9e848709878fcb757}{%
           family={Prettenhofer},
           family_i={P\bibinitperiod},
           given={P.},
           given_i={P\bibinitperiod}}}%
        {{hash=56ac3785f81550a29a28d3abd746ecff}{%
           family={Weiss},
           family_i={W\bibinitperiod},
           given={R.},
           given_i={R\bibinitperiod}}}%
        {{hash=cb070bdb80f3a6add15c62ca13a4a419}{%
           family={Dubourg},
           family_i={D\bibinitperiod},
           given={V.},
           given_i={V\bibinitperiod}}}%
        {{hash=f1aa9e81c02f406a26762c8c85fdc1fb}{%
           family={Vanderplas},
           family_i={V\bibinitperiod},
           given={J.},
           given_i={J\bibinitperiod}}}%
        {{hash=35a7037441ee9fd7ca45c2216a9cb080}{%
           family={Passos},
           family_i={P\bibinitperiod},
           given={A.},
           given_i={A\bibinitperiod}}}%
        {{hash=39b253ebe84b96f6acd07b762d251ca1}{%
           family={Cournapeau},
           family_i={C\bibinitperiod},
           given={D.},
           given_i={D\bibinitperiod}}}%
        {{hash=3c836162a7c1b02a3454bb2df2c57303}{%
           family={Brucher},
           family_i={B\bibinitperiod},
           given={M.},
           given_i={M\bibinitperiod}}}%
        {{hash=980c432be67e1ca5879a0aee315ff2a9}{%
           family={Perrot},
           family_i={P\bibinitperiod},
           given={M.},
           given_i={M\bibinitperiod}}}%
        {{hash=ad11ee93370bf63bb64684cfdff0836f}{%
           family={Duchesnay},
           family_i={D\bibinitperiod},
           given={E.},
           given_i={E\bibinitperiod}}}%
      }
      \strng{namehash}{7f03c9665977d16cca3f7d744a8a06df}
      \strng{fullhash}{81371d66ce2cfb28d217f955948ace55}
      \field{sortinit}{P}
      \field{sortinithash}{c0a4896d0e424f9ca4d7f14f2b3428e7}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{Journal of Machine Learning Research}
      \field{title}{Scikit-learn: Machine Learning in {P}ython}
      \field{volume}{12}
      \field{year}{2011}
      \field{pages}{2825\bibrangedash 2830}
      \range{pages}{6}
    \endentry
    \entry{Smola2004}{article}{}
      \name{author}{3}{}{%
        {{hash=d958fd72cbc7963e7ed0e0fc2f97f282}{%
           family={Smola},
           family_i={S\bibinitperiod},
           given={Alex\bibnamedelima J},
           given_i={A\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
        {{hash=e1ab01ba70effe26d0117b2395fad069}{%
           family={Sch},
           family_i={S\bibinitperiod},
           given={Bernhard},
           given_i={B\bibinitperiod}}}%
        {{hash=7c06bbe61ea42603832b246c6c4c20a4}{%
           family={Schölkopf},
           family_i={S\bibinitperiod},
           given={B},
           given_i={B\bibinitperiod}}}%
      }
      \strng{namehash}{a06f7d1b270755cfabbc636a37c18b7c}
      \strng{fullhash}{a06f7d1b270755cfabbc636a37c18b7c}
      \field{sortinit}{S}
      \field{sortinithash}{fd1e7c5ab79596b13dbbb67f8d70fb5a}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In this tutorial we give an overview of the basic ideas underlying Support Vector (SV) machines for function estimation. Furthermore, we include a summary of currently used algorithms for training SV machines, covering both the quadratic (or convex) programming part and advanced methods for dealing with large datasets. Finally, we mention some modifications and extensions that have been applied to the standard SV algorithm, and discuss the aspect of regularization from a SV perspective.Smola, A. J., Sch, B., {\&} Schölkopf, B. (2004). A Tutorial on Support Vector Regression. Statistics and Computing, 14(3), 199–222. doi:10.1023/B:STCO.0000035301.49549.88}
      \field{isbn}{0960-3174}
      \field{issn}{09603174}
      \field{journaltitle}{Statistics and Computing}
      \field{number}{3}
      \field{title}{{A Tutorial on Support Vector Regression}}
      \field{volume}{14}
      \field{year}{2004}
      \field{pages}{199\bibrangedash 222}
      \range{pages}{24}
      \verb{doi}
      \verb 10.1023/B:STCO.0000035301.49549.88
      \endverb
      \verb{file}
      \verb :Users/sergeyivanychev/Dropbox/Work/6 term/paper/papers/SmoSch03b.pdf:pdf
      \endverb
      \keyw{SVM,SVR,Support Vector Regression}
    \endentry
    \entry{Wolpert}{article}{}
      \name{author}{1}{}{%
        {{hash=d021247d69e9ad159b144eaf4c2be075}{%
           family={Wolpert},
           family_i={W\bibinitperiod},
           given={David\bibnamedelima H},
           given_i={D\bibinitperiod\bibinitdelim H\bibinitperiod}}}%
      }
      \strng{namehash}{d021247d69e9ad159b144eaf4c2be075}
      \strng{fullhash}{d021247d69e9ad159b144eaf4c2be075}
      \field{sortinit}{W}
      \field{sortinithash}{99e3ba1b3f78bb6f073e7fa7ac11636b}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This paper introduces stacked generalization, a scheme for minimizing the generalization error rate of one or more generalizers. Stacked generalization works by deducing the biases of the generalizer(s) with respect to a provided learning set. This deduction proceeds by generalizing in a second space whose inputs are (for example) the guesses of the original generalizers when taught with part of the learning set and trying to guess the rest of it, and whose output is (for example) the correct guess. When used with multiple generalizers, stacked generalization can be seen as a more sophisticated version of cross-validation, exploiting a strategy more sophisticated than cross-vali-dation's crude winner-takes-all for combining the individual generalizers. When used with a single generalizer, stacked generalization is a scheme for estimating (and then correcting for) the error of a generalizer which has been trained on a particular learning set and then asked a particular ques-tion. After introducing stacked generalization and justifying its use, this paper presents two numer-ical experiments. The first demonstrates how stacked generalization improves upon a set of sepa-rate generalizers for the NETtalk task of translating text to phonemes. The second demonstrates how stacked generalization improves the performance of a single surface-fitter. With the other ex-perimental evidence in the literature, the usual arguments supporting cross-validation, and the ab-stract justifications presented in this paper, the conclusion is that for almost any real-world gener-alization problem one should use some version of stacked generalization to minimize the general-ization error rate. This paper ends by discussing some of the variations of stacked generalization, and how it touches on other fields like chaos theory.}
      \field{number}{505}
      \field{title}{{Stacked Generalization}}
      \field{volume}{87545}
      \verb{file}
      \verb :Users/iv/Desktop/Wolpert1992.pdf:pdf
      \endverb
      \keyw{combining generalizers,cross-validation,error estimation and correction,generalization and induction,learning set pre-processing}
    \endentry
  \endsortlist
\endrefsection
\endinput


\documentclass[12pt,twoside]{article}
\usepackage{jmlda}
\usepackage{hyperref}       % clickable links
\usepackage{Iv_commands}    % you can deletee this
\newcommand{\hdir}{.}
\renewcommand{\L}{\mathcal{L}}

% version 0.3

\title
    {Синергия алгоритмов классификации (SVM Multimodelling)}
\author
    {С.\,~Иванычев, А.\,~Адуенко} 
\email
    {\href{mailto:sergeyivanychev@gmail.com}{sergeyivanychev@gmail.com},  \href{mailto:aduenko1@gmail.com}{aduenko1@gmail.com}}
\organization
    {Московский физико-технический институт}
\abstract
    {В данной статье рассматривается проблема агрегирования небольшого количества сильных классификаторов с целью улучшения решений задач классификации и регрессии.  В качестве примера подобной системы рассматривается система SVM  алгоритмов использующая kernel-trick с различными ядрами. Для комбинации решений и улучшения качества прогнозирования в задачах классификации и регрессии (SVR) авторы предлагают способ формирования новых признаков на основе сгенерированных отступов (\emph{margins}) каждым классификатором, приводят алгоритм обучения на полученных объектах и анализируют отличия множеств опорных объектов для различных ядер. В качестве практической проверки были проведены эксперименты на различных реальных данных из репозитория UCI.
    
    \bigskip
\noindent
\textbf{Ключевые слова}: \emph {двухклассовая классификация, композиция алгоритмов, SVM, SVR, бэггинг} 
}

\bibliographystyle{unsrt}

\begin{document}
\maketitle

\section{Введение}
Работа посвящена комбинированию небольшого количества сильных SVM, использующих kernel-trick с различными ядрами и получению агрегированного классификатора для улучшения решений задач классификации и регрессии. 

    SVM(Support Vector Machine) или \emph{метод опорных векторов}\cite{Vapnik1998}\cite{Cortes1995} \cite{Boser1992} ---~это один из наиболее распространенных и эффективных методов в машинном обучении, которые используется для задач классификации и регрессии (SVR). Задача математического программирования сводится к двойственной задаче, функционалы в которой не зависят от векторов признаков как таковых, а лишь от их попарных скалярных произведений \cite{Voron} . Использование особых функций, \emph{ядер}, то есть скалярных произведений в сопряженном пространстве, позволяет получить разделяющие поверхности между классами более сложной формы \cite{Smola2004}. Наша цель --- скомбинировать SVM
    с различными примененными ядрами для улучшения решения, а также анализ множеств опорных объектов в случае разных использованных ядер.
    
    Наиболее классическими методами агрегирования алгоритмов являются 
    бэггинг (\emph{bagging})\cite{Breiman1996} и бустинг (\emph{boosting}) \cite{Freund1995}, и их
    вариации, однако они работают только с  большим количеством слабых классификаторов, что делает невозможным использование его использование для указанного множества базовых алгоритмов.
    
    Среди способов агрегации для небольшого количества классификаторов можно
    выделить, например, выбор большинства классификаторов \cite{Franke1992}, 
    комбинирование ранжирований (rankings) по классам, сделанных различными
    классификаторами \cite{Ho1994}. В дальнейшем было показано, что все подобные 
    методы есть особые случаи составного классификатора из \cite{Kittler1996}, 
    появляющиеся при особых условиях или способах аппроксимации.
    
    Различные способы агрегации SVМ используются во многих задачах анализа данных. 
    \cite{Martin-merino2007} использовали совокупность SVМ для уменьшения ошибочно негативных классификаций (FP) в задаче фильтрации спама среди электронных писем. 
    Для этого на электронных письмах были введены различные метрики, для каждой из них был приспособлен SVM, а затем результат получался голосованием \cite{Kittler1996}. 
    \cite{Gorgevik2005}, решавшие задачу распознавания написанных рукой символов, делили множество признаков на четыре непересекающихся подмножества, и на каждом из них обучали SVM, увеличив этим самым коэффициент распознавания по сравнению с одним SVM.
    
    В последнее время стал набирать популярность \emph{метод многоядерного обучения} (MKL, multiple kernel learning) \cite{Dyrba2015} \cite{Bucak2014}\cite{Althloothi2014}, который основывается на том, что линейная комбинация ядер также является ядром. Данный метод хорош при объединении данных из нескольких источников и полной автоматизации, так как суперпозиция функций может быть оптимизирована любым методом валидации (например кросс-валидацией).
         
    Мы также предлагаем использовать накопившийся банк ядер, однако не на этапе обучения SVM, а на этапе агрегирования обученных алгоритмов. Известно, что алгоритм $b_i$ для объекта $x_j$ обучающей выборки генерирует \emph{отступ} (margin). По отступу в общем случае можно определить не только предсказанный класс, но и насколько <<уверен>> в своем решении алгоритм. В случае банка с $n$ ядрами и обучающей выборки с $m$ сэмплами
    мы получим матрицу отступов $M \in \R^{m\times n}$. Отнормировав ее, мы получим новую матрицу <<объект-признак>>, где вектором признаков каждого объекта будет вектор отнормированных отступов.
    
    В этой работе предложен алгоритм обучения на матрице отступов, проведен анализ опорных объектов, генерируемые различными ядрами, а также проведено тестирование полученного алгоритма на реальных данных репозитория UCI. 


\section{Постановка задачи}

Пусть $\mathcal{K} = \{K_i\}_{i=1}^m$ ---~множество ядер, выбранных для
мультимоделирования SVM, $X^l = \brs{x_i, y_i}_{i=1}^l$~---~обучающая выборка$, x\in \R^n, y \in Y$. Тогда множество обученных SVM на данной выборке:

\begin{equation}
    \mathcal{B} = \fbrs{b_i| b_i = b_i(x) = \sign{\sum_{i=1}^l\lambda_i y_i K_i(x_i, x) - w_0}\}_{i=1}^m}
\end{equation}

Где $\lambda_s$ находятся из решения задачи математического программирования

\begin{equation*}
 \begin{cases}
   \sum_{i=1}^l \lambda_i + \frac{1}{2}\sum_{i=1}^{l}\sum_{j=1}^{l}
    \lambda_i \lambda_j y_i y_j K_s(x_i, x_j) \to \min_\lambda
   \\
   0 \leq \lambda_s \leq c, \;\; i = 1\ldots l
   \\
   \sum_{i=1}^l\lambda_i y_i = 0
 \end{cases}
\end{equation*}

И $w_0 = \sum_{i=1}^l \lambda_i y_i K_s(x_i, x_j) - y_j$ для такого $j$, что 
$\lambda_j > 0, M_j = 1$. Паре <<выборка-обученные алгоритмы>> соответствует матрица отступов размерности 
$M \in \R^{l\times m}$,
в котором $(i, j)$-й элемент ---~это отступ $i$-го объекта в SVM с~$j$-м ядром.
Пусть $M$ --- матрица <<объект-признак>>, $\mathcal{A}$ ---~ множество алгоритмов
вида

\begin{equation}
    \mathcal{A} = \fbrs{a(x) = g(x, \theta)|\theta \in \Theta}\;\; g:\R^m \to Y
\end{equation}

$\L(y, y^*)$ ---~функционал качества, тогда перед нами стоит задача минимизации

\begin{equation}
    L(y, g(M, \theta)) \to \min_{\mathcal{A}, \Theta}
\end{equation}


\nocite{*}
\bibliography{papers}

\end{document}

@article{Boser1992,
abstract = {A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented. The technique is applicable to a wide variety of classifiaction functions, including Perceptrons, polynomials, and Radial Basis Functions. The effective number of parameters is adjusted automatically to match the complexity of the problem. The solution is expressed as a linear combination of supporting patterns. These are the subset of training patterns that are closest to the decision boundary. Bounds on the generalization performance based on the leave-one-out method and the VC-dimension are given. Experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning algorithms. 1 INTRODUCTION Good generalization performance of pattern classifiers is achieved when the capacity of the classification function is matched to the size of the training set. Classifiers with a large numb...},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Boser, Bernhard E. and Guyon, Isabelle M. and Vapnik, Vladimir N.},
doi = {10.1.1.21.3818},
eprint = {arXiv:1011.1669v3},
file = {:Users/sergeyivanychev/Library/Application Support/Mendeley Desktop/Downloaded/Boser, Guyon, Vapnik - 1992 - A Training Algorithm for Optimal Margin Classifiers.pdf:pdf},
isbn = {089791497X},
issn = {0-89791-497-X},
journal = {Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory},
pages = {144--152},
pmid = {25246403},
title = {{A Training Algorithm for Optimal Margin Classifiers}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.21.3818},
year = {1992}
}
@article{Ho1994,
author = {Ho, Tin Kam T.K. and Hull, JJ and Srihari, Sargur N SN and Member, Senior},
file = {:Users/sergeyivanychev/Dropbox/Work/6 term/paper/papers/Decision combination.pdf:pdf},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {1},
pages = {66--75},
title = {{Decision combination in multiple classifier systems}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Decision+Combination+in+Multiple+Classifier+Systems{\#}0},
volume = {16},
year = {1994}
}
@article{Cortes1995,
abstract = {Oil/water partition coefficient (log P) is one of the key points for lead compound to be drug. In silico log P models based solely on chemical structures have become an important part of modern drug discovery. Here, we report support vector machines, radial basis function neural networks, and multiple linear regression methods to investigate the correlation between partition coefficient and physico-chemical descriptors for a large data set of compounds. The correlation coefficient r (2) between experimental and predicted log P for training and test sets by support vector machines, radial basis function neural networks, and multiple linear regression is 0.92, 0.90, and 0.88, respectively. The results show that non-linear support vector machines derives statistical models that have better prediction ability than those of radial basis function neural networks and multiple linear regression methods. This indicates that support vector machines can be used as an alternative modeling tool for quantitative structure-property/activity relationships studies.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Cortes, Corinna and Vapnik, Vladimir},
doi = {10.1023/A:1022627411411},
eprint = {arXiv:1011.1669v3},
isbn = {0885-6125},
issn = {15730565},
journal = {Machine Learning},
keywords = {efficient learning algorithms,neural networks,pattern recognition,polynomial classifiers,radial basis function classifiers},
number = {3},
pages = {273--297},
pmid = {19549084},
title = {{Support-Vector Networks}},
volume = {20},
year = {1995}
}
@article{Kittler1996,
abstract = {We develop a common theoretical framework for combining$\backslash$nclassifiers which use distinct pattern representations and show that$\backslash$nmany existing schemes can be considered as special cases of compound$\backslash$nclassification where all the pattern representations are used jointly to$\backslash$nmake a decision. An experimental comparison of various classifier$\backslash$ncombination schemes demonstrates that the combination rule developed$\backslash$nunder the most restrictive assumptions-the sum rule-outperforms other$\backslash$nclassifier combinations schemes. A sensitivity analysis of the various$\backslash$nschemes to estimation errors is carried out to show that this finding$\backslash$ncan be justified theoretically},
author = {Kittler, J. and Hater, M. and Duin, R. P W},
doi = {10.1109/ICPR.1996.547205},
file = {:Users/sergeyivanychev/Dropbox/Work/6 term/paper/papers/kittler1998.pdf:pdf},
isbn = {081867282X},
issn = {10514651},
journal = {Proceedings - International Conference on Pattern Recognition},
number = {3},
pages = {897--901},
pmid = {20470429},
title = {{Combining classifiers}},
volume = {2},
year = {1996}
}
@article{Breiman1996,
abstract = {Bagging predictors is a method for generating multiple versions of a pre-dictor and using these to get an aggregated predictor. The aggregation av-erages over the versions when predicting a numerical outcome and does a plurality v ote when predicting a class. The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. Tests on real and simulated data sets using classiication and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy. The vital element is the instability o f the prediction method. If perturbing the learning set can cause signiicant changes in the predictor constructed, then bagging can improve accuracy.},
author = {Breiman, Leo},
doi = {10.1007/BF00058655},
file = {:Users/sergeyivanychev/Library/Application Support/Mendeley Desktop/Downloaded/Breiman - 1996 - Bagging Predictors.pdf:pdf},
isbn = {0885-6125},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {aggregation,averaging,bootstrap,combining},
number = {421},
pages = {123--140},
pmid = {17634459},
title = {{Bagging Predictors}},
volume = {24},
year = {1996}
}
@article{Vapnik1998,
abstract = {A comprehensive look at learning and generalization theory. The statistical theory of learning and generalization concerns the problem of choosing desired functions on the basis of empirical data. Highly applicable to a variety of computer science and robotics fields, this book offers lucid coverage of the theory as a whole. Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real-life problems, and much more.},
author = {Vapnik, Vladimir N},
doi = {10.2307/1271368},
file = {:Users/sergeyivanychev/Dropbox/Work/6 term/paper/papers/Statistical-Learning-Theory.pdf:pdf},
isbn = {0-471-03003-1},
issn = {10762787},
journal = {Adaptive and learning Systems for Signal Processing, Communications and Control},
pages = {1--740},
pmid = {20210330},
title = {{Statistical Learning Theory}},
url = {http://eu.wiley.com/WileyCDA/WileyTitle/productCd-0471030031.html},
volume = {2},
year = {1998}
}
@article{Amari1999,
abstract = {We propose a method of modifying a kernel function to improve the performance of a support vector machine classifier. This is based on the structure of the Riemannian geometry induced by the kernel function. The idea is to enlarge the spatial resolution around the separating boundary surface, by a conformal mapping, such that the separability between classes is increased. Examples are given specifically for modifying Gaussian Radial Basis Function kernels. Simulation results for both artificial and real data show remarkable improvement of generalization errors, supporting our idea. Copyright (C) 1999 Elsevier Science Ltd.},
author = {Amari, S. and Wu, S.},
doi = {10.1016/S0893-6080(99)00032-5},
file = {:Users/sergeyivanychev/Dropbox/Work/6 term/paper/papers/amari1999.pdf:pdf},
isbn = {0893-6080},
issn = {08936080},
journal = {Neural Networks},
keywords = {Information geometry,Kernel Adatron,Kernel function,Nonlinear classification,Pattern classification,Radial basis function,Riemannian geometry,Support vector machine},
number = {6},
pages = {783--789},
pmid = {12662656},
title = {{Improving support vector machine classifiers by modifying kernel functions}},
volume = {12},
year = {1999}
}
@article{Shipp2002,
abstract = {This study looks at the relationships between different methods of classifier combination and different measures of diversity. We considered 10 combination methods and 10 measures of diversity on two benchmark data sets. The relationship was sought on ensembles of three classifiers built on all possible partitions of the respective feature sets into subsets of pre-specified sizes. The only positive finding was that the Double-Fault measure of diversity and the measure of difficulty both showed reasonable correlation with Majority Vote and Naive Bayes combinations. Since both these measures have an indirect connection to the ensemble accuracy, this result was not unexpected. However, our experiments did not detect a consistent relationship between the other measures of diversity and the 10 combination methods. ?? 2002 Published by Elsevier Science B.V.},
author = {Shipp, Catherine A. and Kuncheva, Ludmila I.},
doi = {10.1016/S1566-2535(02)00051-9},
file = {:Users/sergeyivanychev/Dropbox/Work/6 term/paper/papers/Relationships between combination methods and measures of diversity in combining classifiers.pdf:pdf},
isbn = {1566-2535},
issn = {15662535},
journal = {Information Fusion},
keywords = {Combining classifier,Dependence,Diversity},
number = {2},
pages = {135--148},
title = {{Relationships between combination methods and measures of diversity in combining classifiers}},
volume = {3},
year = {2002}
}
@article{Smola2004,
abstract = {In this tutorial we give an overview of the basic ideas underlying Support Vector (SV) machines for function estimation. Furthermore, we include a summary of currently used algorithms for training SV machines, covering both the quadratic (or convex) programming part and advanced methods for dealing with large datasets. Finally, we mention some modifications and extensions that have been applied to the standard SV algorithm, and discuss the aspect of regularization from a SV perspective.Smola, A. J., Sch, B., {\&} Sch{\"{o}}lkopf, B. (2004). A Tutorial on Support Vector Regression. Statistics and Computing, 14(3), 199–222. doi:10.1023/B:STCO.0000035301.49549.88},
author = {Smola, Alex J and Sch, Bernhard and Sch{\"{o}}lkopf, B},
doi = {10.1023/B:STCO.0000035301.49549.88},
file = {:Users/sergeyivanychev/Dropbox/Work/6 term/paper/papers/SmoSch03b.pdf:pdf},
isbn = {0960-3174},
issn = {09603174},
journal = {Statistics and Computing},
keywords = {SVM,SVR,Support Vector Regression},
number = {3},
pages = {199--222},
pmid = {17969693},
title = {{A Tutorial on Support Vector Regression}},
volume = {14},
year = {2004}
}
@article{Gorgevik2005,
abstract = {Recent results in pattern recognition have shown that SVM (support vector machine) classifiers often have superior recognition rates in comparison to other classification methods. In this paper, a cooperation of four SVM classifiers for handwritten digit recognition, each using different feature set is examined. We investigate the advantages and weaknesses of various cooperation schemes based on classifier decision fusion using statistical reasoning. The obtained results show that it is difficult to exceed the recognition rate of a single, well-tuned SVM classifier applied straightforwardly on all feature sets. In our experiments only one of the cooperation schemes exceeds the recognition rate of a single SVM classifier. However, the classifier cooperation reduces the classifier complexity and need for training samples, decreases classifier training time and sometimes improves the classifier performance},
author = {Gorgevik, D. and Cakmakov, D.},
doi = {10.1109/EURCON.2005.1630221},
file = {:Users/sergeyivanychev/Dropbox/Work/6 term/paper/papers/Gorgevik.pdf:pdf},
isbn = {1-4244-0049-X},
journal = {EUROCON 2005 - The International Conference on Computer as a Tool""},
keywords = {classifier,decision fusion,features,statistical},
number = {February},
pages = {1393--1396},
title = {{Handwritten Digit Recognition by Combining SVM Classifiers}},
volume = {2},
year = {2005}
}
@article{Fei2006,
abstract = {We present a new architecture named Binary Tree of support vector machine (SVM), or BTS, in order to achieve high classification efficiency for multiclass problems. BTS and its enhanced version, c-BTS, decrease the number of binary classifiers to the greatest extent without increasing the complexity of the original problem. In the training phase, BTS has N - 1 binary classifiers in the best situation (N is the number of classes), while it has log4/3 ((N + 3)/4) binary tests on average when making a decision. At the same time the upper bound of convergence complexity is determined. The experiments in this paper indicate that maintaining comparable accuracy, BTS is much faster to be trained than other methods. Especially in classification, due to its Log complexity, it is much faster than directed acyclic graph SVM (DAGSVM) and ECOC in problems that have big class number.},
author = {Fei, Ben and Liu, Jinbai},
doi = {10.1109/TNN.2006.872343},
file = {:Users/sergeyivanychev/Dropbox/Work/6 term/paper/papers/fei2006.pdf:pdf},
issn = {10459227},
journal = {IEEE Transactions on Neural Networks},
keywords = {Binary tree of support vector machine (BTS),Multiclass classification,Probabilistic output,Support vector machine (SVM),c-BTS},
number = {3},
pages = {696--704},
pmid = {16722173},
title = {{Binary tree of SVM: A new fast multiclass training and classification algorithm}},
volume = {17},
year = {2006}
}
@article{Martin-merino2007,
author = {Martin-merino, Manuel and Mart, Manuel},
doi = {10.1007/978-3-540-73007-1},
file = {:Users/sergeyivanychev/Dropbox/Work/6 term/paper/papers/Blanco.pdf:pdf},
isbn = {978-3-540-73006-4},
number = {February},
title = {{Combibing SVM Classifiers for Email Anti-spam Filtering}},
url = {http://link.springer.com/10.1007/978-3-540-73007-1},
volume = {4507},
year = {2007}
}
@article{Vapnik2009,
abstract = {In the Afterword to the second edition of the book "Estimation of Dependences Based on Empirical Data" by V. Vapnik, an advanced learning paradigm called Learning Using Hidden Information (LUHI) was introduced. This Afterword also suggested an extension of the SVM method (the so called SVM(gamma)+ method) to implement algorithms which address the LUHI paradigm (Vapnik, 1982-2006, Sections 2.4.2 and 2.5.3 of the Afterword). See also (Vapnik, Vashist, {\&} Pavlovitch, 2008, 2009) for further development of the algorithms. In contrast to the existing machine learning paradigm where a teacher does not play an important role, the advanced learning paradigm considers some elements of human teaching. In the new paradigm along with examples, a teacher can provide students with hidden information that exists in explanations, comments, comparisons, and so on. This paper discusses details of the new paradigm and corresponding algorithms, introduces some new algorithms, considers several specific forms of privileged information, demonstrates superiority of the new learning paradigm over the classical learning paradigm when solving practical problems, and discusses general questions related to the new ideas.},
author = {Vapnik, Vladimir and Vashist, Akshay},
doi = {10.1016/j.neunet.2009.06.042},
issn = {1879-2782},
journal = {Neural networks : the official journal of the International Neural Network Society},
keywords = {Algorithms,Artificial Intelligence,Bayes Theorem,Databases, Genetic,Forecasting,Forecasting: methods,Information Dissemination,Language,Learning,Mathematical Concepts,Protein Conformation,Proteins,Proteins: classification,Sequence Analysis, Protein,Sequence Analysis, Protein: methods,Time Factors},
month = {jan},
number = {5-6},
pages = {544--57},
pmid = {19632812},
title = {{A new learning paradigm: learning using privileged information.}},
url = {http://www.sciencedirect.com/science/article/pii/S0893608009001130},
volume = {22},
year = {2009}
}
@article{Althloothi2014,
abstract = {This paper presents two sets of features, shape representation and kinematic structure, for human activity recognition using a sequence of RGB-D images. The shape features are extracted using the depth information in the frequency domain via spherical harmonics representation. The other features include the motion of the 3D joint positions (i.e. the end points of the distal limb segments) in the human body. Both sets of features are fused using the Multiple Kernel Learning (MKL) technique at the kernel level for human activity recognition. Our experiments on three publicly available datasets demonstrate that the proposed features are robust for human activity recognition and particularly when there are similarities among the actions. ?? 2013 Elsevier Ltd. All rights reserved.},
author = {Althloothi, Salah and Mahoor, Mohammad H. and Zhang, Xiao and Voyles, Richard M.},
doi = {10.1016/j.patcog.2013.11.032},
file = {:Users/sergeyivanychev/Library/Application Support/Mendeley Desktop/Downloaded/Althloothi et al. - 2014 - Human activity recognition using multi-features and multiple kernel learning.pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Distal limb segments,Human activity recognition,Multiple kernel learning,Spherical harmonics coefficients,Support vector machines},
number = {5},
pages = {1800--1812},
publisher = {Elsevier},
title = {{Human activity recognition using multi-features and multiple kernel learning}},
url = {http://dx.doi.org/10.1016/j.patcog.2013.11.032},
volume = {47},
year = {2014}
}
@article{Bucak2014,
abstract = {Multiple kernel learning (MKL) is a principled approach for selecting and combining kernels for a given recognition task. A number of studies have shown that MKL is a useful tool for object recognition, where each image is represented by multiple sets of features and MKL is applied to combine different feature sets. We review the state-of-the-art for MKL, including different formulations and algorithms for solving the related optimization problems, with the focus on their applications to object recognition. One dilemma faced by practitioners interested in using MKL for object recognition is that different studies often provide conflicting results about the effectiveness and efficiency of MKL. To resolve this, we conduct extensive experiments on standard datasets to evaluate various approaches to MKL for object recognition. We argue that the seemingly contradictory conclusions offered by studies are due to different experimental setups. The conclusions of our study are: (i) given a sufficient number of training examples and feature/kernel types, MKL is more effective for object recognition than simple kernel combination (e.g., choosing the best performing kernel or average of kernels); and (ii) among the various approaches proposed for MKL, the sequential minimal optimization, semi-infinite programming, and level method based ones are computationally most efficient.},
author = {Bucak, S.S. and Jin, R. and Jain, Ak.},
doi = {10.1109/TPAMI.2013.212},
file = {:Users/sergeyivanychev/Library/Application Support/Mendeley Desktop/Downloaded/Bucak, Jin, Jain - 2014 - Multiple Kernel Learning for Visual Object Recognition A Review.pdf:pdf},
isbn = {0162-8828 VO  - 36},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Feature evaluation and selection,Histograms,Image color analysis,Introductory and Survey,Kernel,Machine learning,Multiple kernel learning,Object recognition,Optimization,Training,Visualization,convex optimization,support vector machine,visual object recognition},
number = {7},
pages = {1354--1369},
pmid = {26353308},
title = {{Multiple Kernel Learning for Visual Object Recognition: A Review}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6654166},
volume = {36},
year = {2014}
}
@article{Dyrba2015,
abstract = {Alzheimer's disease (AD) patients exhibit alterations in the functional connectivity between spatially segregated brain regions which may be related to both local gray matter (GM) atrophy as well as a decline in the fiber integrity of the underlying white matter tracts. Machine learning algorithms are able to automatically detect the patterns of the disease in image data, and therefore, constitute a suitable basis for automated image diagnostic systems. The question of which magnetic resonance imaging (MRI) modalities are most useful in a clinical context is as yet unresolved. We examined multimodal MRI data acquired from 28 subjects with clinically probable AD and 25 healthy controls. Specifically, we used fiber tract integrity as measured by diffusion tensor imaging (DTI), GM volume derived from structural MRI, and the graph-theoretical measures 'local clustering coefficient' and 'shortest path length' derived from resting-state functional MRI (rs-fMRI) to evaluate the utility of the three imaging methods in automated multimodal image diagnostics, to assess their individual performance, and the level of concordance between them. We ran the support vector machine (SVM) algorithm and validated the results using leave-one-out cross-validation. For the single imaging modalities, we obtained an area under the curve (AUC) of 80{\%} for rs-fMRI, 87{\%} for DTI, and 86{\%} for GM volume. When it came to the multimodal SVM, we obtained an AUC of 82{\%} using all three modalities, and 89{\%} using only DTI measures and GM volume. Combined multimodal imaging data did not significantly improve classification accuracy compared to the best single measures alone. Hum Brain Mapp 36:2118-2131, 2015. © 2015 Wiley Periodicals, Inc.},
author = {Dyrba, Martin and Grothe, Michel and Kirste, Thomas and Teipel, Stefan J.},
doi = {10.1002/hbm.22759},
file = {:Users/sergeyivanychev/Library/Application Support/Mendeley Desktop/Downloaded/Dyrba et al. - 2015 - Multimodal analysis of functional and structural disconnection in Alzheimer's disease using multiple kernel SVM.pdf:pdf},
issn = {10970193},
journal = {Human Brain Mapping},
keywords = {Alzheimer's disease,Diffusion tensor imaging,Magnetic resonance imaging,Multiple kernel support vector machine,Resting-state functional magnetic resonance imagin},
number = {6},
pages = {2118--2131},
pmid = {25664619},
title = {{Multimodal analysis of functional and structural disconnection in Alzheimer's disease using multiple kernel SVM}},
volume = {36},
year = {2015}
}
@article{Signal2015,
author = {Signal, European and Conference, Processing},
file = {:Users/sergeyivanychev/Library/Application Support/Mendeley Desktop/Downloaded/Signal, Conference - 2015 - BOOTSTRAP-BASED SVM AGGREGATION FOR CLASS IMBALANCE PROBLEMS S . Sukhanov , A . Merentitis , C . Debes AGT.pdf:pdf},
isbn = {9780992862633},
pages = {165--169},
title = {{BOOTSTRAP-BASED SVM AGGREGATION FOR CLASS IMBALANCE PROBLEMS S . Sukhanov , A . Merentitis , C . Debes AGT International J . Hahn , A . M . Zoubir Signal Processing Group Technische Universit at Darmstadt , Germany}},
year = {2015}
}
@article{Voron,
author = {Vorontsov, Konstantin Vyacheslavovich},
title = {{Mathematical methods of learning from examples (machine lerning theory)}},
url = {www.machinelearning.ru/wiki/images/6/6d/Voron-ML-1.pdf}
}
@misc{Freund1995,
abstract = {We present an algorithm for improving the accuracy of algorithms for learning binary concepts. The improvement is achieved by combining a large number of hypotheses, each of which is generated by training the given learning algorithm on a different set of examples. Our algorithm is based on ideas presented by Schapire and represents an improvement over his results, The analysis of our algorithm provides general upper bounds on the resources required for learning in Valiants polynomial PAC learning framework, which are the best general upper bounds known today. We show that the number of hypotheses that are combined by our algorithm is the smallest number possible. Other outcomes of our analysis are results regarding the representational power of threshold circuits, the relation between learnability and compression, and a method for parallelizing PAC learning algorithms. We provide extensions of our algorithms to cases in which the concepts are not binary and to the case where the accuracy of the learning algorithm depends on the distribution of the instances.},
author = {Freund, Y.},
booktitle = {Information and Computation},
doi = {10.1006/inco.1995.1136},
file = {:Users/sergeyivanychev/Dropbox/Work/6 term/paper/papers/base/boosting.pdf:pdf},
isbn = {1558601465},
issn = {08905401},
number = {2},
pages = {256--285},
title = {{Boosting a Weak Learning Algorithm by Majority}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0890540185711364},
volume = {121},
year = {1995}
}
@article{Franke1992,
abstract = {Two different approaches are described to combine the results of$\backslash$ndifferent classifiers. The first approach is based on the$\backslash$nDempster/Shafer theory of evidence and the second one is a statistical$\backslash$napproach with some assumptions on the input data. Both approaches were$\backslash$ntested for user-dependent recognition of on-line handwritten characters$\backslash$n},
author = {Franke, J. and Mandler, E.},
doi = {10.1109/ICPR.1992.201786},
file = {:Users/sergeyivanychev/Dropbox/Work/6 term/paper/papers/franke1992.pdf:pdf},
isbn = {0-8186-2915-0},
journal = {Proceedings., 11th IAPR International Conference on Pattern Recognition. Vol.II. Conference B: Pattern Recognition Methodology and Systems},
pages = {1--4},
title = {{A comparison of two approaches for combining the votes of$\backslash$ncooperating classifiers}},
year = {1992}
}

1. Hungarian Institute of Cardiology. Budapest: Andras Janosi, M.D.
2. University Hospital, Zurich, Switzerland: William Steinbrunn, M.D.
3. University Hospital, Basel, Switzerland: Matthias Pfisterer, M.D.
4. V.A. Medical Center, Long Beach and Cleveland Clinic Foundation:Robert Detrano, M.D., Ph.D.

@misc{UCI:Heart ,
author = "Andras Janosi M.D and William Steinbrunn, M.D and
Matthias Pfisterer, M.D and Robert Detrano, M.D, Ph.D",
year = "1988",
title = "Heart Disease Data Set",
url = "https://archive.ics.uci.edu/ml/datasets/Heart+Disease",
institution = "Hungarian Institute of Cardiology, Budapest;
University Hospital, Zurich; University Hospital, Basel;
V.A. Medical Center, Long Beach and Cleveland Clinic Foundation" }

@misc{UCI:German ,
author = "Dr. Hans Hofmann",
year = "1994",
title = "Statlog (German Credit Data) Data Set ",
url = "https://archive.ics.uci.edu/ml/datasets/Statlog+(German+Credit+Data)",
institution = "Institut fur Statistik und Okonometrie" }

@misc{UCI:Wine ,
author = "Stefan Aeberhard",
year = "1991",
title = "Wine Data Set ",
url = "https://archive.ics.uci.edu/ml/datasets/Wine",
institution = "Institute of Pharmaceutical and Food Analysis and Technologies" }
